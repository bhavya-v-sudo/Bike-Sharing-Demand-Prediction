<p align="center"> 
  <img src="Bike-Sharing-Demand-1194x501.jpg" alt="Email Logo.png" width="800px" height="400px">
</p>
<h1 align="center"> Bike Sharing Demand Prediction </h1>
<h3 align="center"> AlmaBetter Verfied Project - <a href="https://www.almabetter.com/"> AlmaBetter School </a> </h5>

<p align="center"> 
<img src="gif/spam detector.gif" alt="Animated gif pacman game" height="382px">
</p>

<p>In this project, I train a model to predict the number of bike rentals at any hour of the year given the weather conditions. The data set was obtained from the Capital Bikeshare program in Washington, D.C. which contained the historical bike usage pattern with weather data spanning two years. I used multiple Regression algorithms - Linear Regression, Ridge Regression, Lasso Regression, Decision Tree, Random forest, and so on.</p>

<h2> :floppy_disk: Project Files Description</h2>

<p>This Project includes 5 files as follows:</p>
<h4>Input Files:</h4>
<ul>
  <li><b>Bike_Sharing_Demand_Prediction_Capstone_Project.ipynb</b> - Includes all functions required for classification operations.</li>
  <li><b>SeoulBikeData.csv</b> - Input CSV file.</li>
</ul>

<h4>Output Files:</h4>
<ul>
  <li><b>CapstoneProjectTemplate.pdf</b> - Powerpoint presentation on the summary of the project.</li>
  <li><b>Technical documentation.pdf</b> - Detailed explanation of the approach of the project.</li>
  <li><b>Project Summary and team details.doc</b> - Project summary and the details on the challenges faced during the project. </li>
</ul>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

<h2> :book: Linear Regression:</h2>

<p>Linear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting. Different regression models differ based on – the kind of relationship between dependent and independent variables they are considering, and the number of independent variables getting used. There are many names for a regression’s dependent variable.  It may be called an outcome variable, criterion variable, endogenous variable, or regressand.  The independent variables can be called exogenous variables, predictor variables, or regressors. </p>

<h2> :book: Lasso Regression:</h2>

<p>Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients. This type of regularization can result in sparse models with few coefficients; Some coefficients can become zero and eliminated from the model. Larger penalties result in coefficient values closer to zero, which is the ideal for producing simpler models. On the other hand, L2 regularization (e.g. Ridge regression) doesn’t result in elimination of coefficients or sparse models. This makes the Lasso far easier to interpret than the Ridge.</p>

<h2> :book: Ridge Regression:</h2>

<p>Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values. </p>

<h2> :book: Decision Tree:</h2>

<p>Decision Tree is the most powerful and popular tool for classification and prediction. A Decision tree is a flowchart-like tree structure, where each internal node denotes a test on an attribute, each branch represents an outcome of the test, and each leaf node (terminal node) holds a class label. </p>

<h2> :book: Random Forest:</h2>

<p>Random Forest is an ensemble technique capable of performing both regression and classification tasks with the use of multiple decision trees and a technique called Bootstrap and Aggregation, commonly known as bagging. The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees.

Random Forest has multiple decision trees as base learning models. We randomly perform row sampling and feature sampling from the dataset forming sample datasets for every model. This part is called Bootstrap.</p>

<h2> :book: XGBoost:</h2>

<p>XGBoost is an implementation of Gradient Boosted decision trees. XGBoost models majorly dominate in many Kaggle Competitions.

In this algorithm, decision trees are created in sequential form. Weights play an important role in XGBoost. Weights are assigned to all the independent variables which are then fed into the decision tree which predicts results. The weight of variables predicted wrong by the tree is increased and these variables are then fed to the second decision tree. These individual classifiers/predictors then ensemble to give a strong and more precise model. It can work on regression, classification, ranking, and user-defined prediction problems.</p>


![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

<!-- CREDITS -->
<h2 id="credits"> :scroll: Credits</h2>

< Bhavya > | Avid Learner | Data Scientist | Machine Learning Engineer | Deep Learning enthusiast

<p> <i> Contact me for Data Science Project Collaborations</i></p>


[![LinkedIn Badge](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/bhavya-reddy-sudo/)
[![GitHub Badge](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/bhavya-v-sudo/Bike-Sharing-Demand-Prediction)
[![Resume Badge](https://img.shields.io/badge/resume-0077B5?style=for-the-badge&logo=resume&logoColor=white)](https://drive.google.com/file/d/1Gw4yeLSWMvqIGMk0_zRv8CqwLk9lIIL9/view?usp=share_link)


![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)
<h2> :books: References</h2>
<ul>
  <li><p>geeksforgeeks.org, 'ML | Linear Regression'. [Online].</p>
      <p>Available: https://www.geeksforgeeks.org/ml-linear-regression/</p>
  </li>
  <li><p>Wikipedia.org, 'Ridge regression'. [Online].</p>
      <p>Available: https://en.wikipedia.org/wiki/Ridge_regression</p>
  </li>
  <li><p>statisticshowto, 'Lasso regression'. [Online].</p>
      <p>Available: https://www.statisticshowto.com/lasso-regression/</p>
  </li>
  <li><p>Youtube.com, 'Decision Tree'. [Online].</p>
      <p>Available: https://www.youtube.com/watch?v=_L39rN6gz7Y</p>
  </li>
  <li><p>javatpoint.com, 'Random forest algorithm'. [Online].</p>
      <p>Available: https://www.javatpoint.com/machine-learning-random-forest-algorithm</p>
  </li>
  <li><p>xgboost.readthedocs.io, 'XGBoost Documentation'. [Online].</p>
      <p>Available: https://xgboost.readthedocs.io/en/stable/</p>
  </li>
</ul>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/rainbow.png)

